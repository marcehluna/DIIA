{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcehluna/DIIA/blob/dev/RAG_GC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDmow-aUea8F"
      },
      "source": [
        "# DIIAA - RAG en Google Cloud\n",
        "\n",
        "## Creando tu propio Asistente RAG con Vertex AI\n",
        "\n",
        "En este cuaderno, aprender√°s a construir un sistema completo de **Retrieval-Augmented Generation (RAG)** desde cero. Al final, tendr√°s un asistente de chat personal que responder√° preguntas bas√°ndose en un documento PDF que t√∫ mismo subir√°s.\n",
        "\n",
        "**Pasos que seguiremos:**\n",
        "1.  **Autenticaci√≥n:** Conectaremos este cuaderno a Google Cloud.\n",
        "2.  **Configuraci√≥n:** Crearemos un \"balde\" (Bucket) para guardar tu archivo.\n",
        "3.  **Carga de Datos:** Subir√°s un PDF y lo guardaremos en la nube.\n",
        "4.  **Creaci√≥n del Cerebro (RAG Engine):** Configuraremos el motor de IA para que lea tu documento.\n",
        "5.  **Chat:** ¬°Haremos preguntas a nuestro nuevo asistente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9ZmEk6yeVO5"
      },
      "outputs": [],
      "source": [
        "# @title Paso 0: Instalar las librer√≠as necesarias\n",
        "# Este comando instala en nuestro entorno de Colab las herramientas (SDKs)\n",
        "# para comunicarnos con los servicios de Google Cloud.\n",
        "# El `-q` es para que la salida sea m√°s limpia (quiet).\n",
        "%pip install --upgrade google-cloud-aiplatform google-cloud-storage google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raDuPBpIe9bR"
      },
      "outputs": [],
      "source": [
        "# @title Paso 1: Autenticaci√≥n con Google Cloud\n",
        "# Para que este cuaderno pueda crear recursos en la nube, necesita permisos.\n",
        "# Usaremos un archivo de credenciales (JSON) que te ha proporcionado el docente.\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Por favor, sube el archivo JSON de credenciales que te dio el docente.\")\n",
        "# Abre una ventana para que puedas seleccionar y subir el archivo.\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Verificamos que se subi√≥ un archivo y lo configuramos como nuestra credencial.\n",
        "if uploaded_files:\n",
        "    key_filename = list(uploaded_files.keys())[0]\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_filename\n",
        "    print(f\"\\n‚úÖ ¬°Autenticaci√≥n exitosa con el archivo '{key_filename}'!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se subi√≥ ning√∫n archivo. Por favor, ejecuta la celda de nuevo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-JfRGNOfO6V"
      },
      "outputs": [],
      "source": [
        "# @title Paso 2: Configuraci√≥n de tu Proyecto\n",
        "# Completa las siguientes variables con tus datos.\n",
        "\n",
        "# 1. Proporcionado por el docente:\n",
        "PROJECT_ID = \"upgrade-hub-433600\"  # Reemplaza con el ID del proyecto de Google Cloud\n",
        "LOCATION = \"us-central1\"               # Reemplaza con la regi√≥n, ej: \"us-central1\"\n",
        "\n",
        "# 2. Tu informaci√≥n:\n",
        "ID_STUDENT = \"mhluna\" # Usa tu n√∫mero de legajo o un identificador √∫nico sin espacios\n",
        "\n",
        "# --- No necesitas modificar nada debajo de esta l√≠nea ---\n",
        "if not PROJECT_ID or \"pega-aqui\" in PROJECT_ID:\n",
        "    print(\"‚ö†Ô∏è ¬°Error! Por favor, edita la variable PROJECT_ID con el valor correcto.\")\n",
        "elif not ID_STUDENT or \"pega-aqui\" in ID_STUDENT:\n",
        "    print(\"‚ö†Ô∏è ¬°Error! Por favor, edita la variable ID_STUDENT con tu identificador.\")\n",
        "else:\n",
        "    # Creamos nombres √∫nicos para nuestros recursos\n",
        "    BUCKET_NAME = \"fiuba-01co2025-diiaa\"\n",
        "    RAG_CORPUS_DISPLAY_NAME = f\"fiuba-01co2025-diiaa-{ID_STUDENT}\"\n",
        "\n",
        "    print(f\"‚úÖ Configuraci√≥n lista para el proyecto '{PROJECT_ID}' y el alumno '{ID_STUDENT}'.\")\n",
        "    print(f\"   - Nombre del Bucket: {BUCKET_NAME}\")\n",
        "    print(f\"   - Nombre del RAG Engine: {RAG_CORPUS_DISPLAY_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3uqybwihob4"
      },
      "outputs": [],
      "source": [
        "# @title Paso 3: Crear el Bucket en Cloud Storage\n",
        "# Un \"bucket\" es como una carpeta en la nube donde guardaremos nuestro PDF.\n",
        "# Este c√≥digo intentar√° crear el bucket. Si ya existe, simplemente nos avisar√° y continuar√°.\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.api_core import exceptions\n",
        "\n",
        "# Nos aseguramos de que las variables del paso 2 est√©n definidas\n",
        "if 'PROJECT_ID' not in locals() or 'BUCKET_NAME' not in locals():\n",
        "    print(\"üö® ¬°Error! Por favor, ejecuta la celda del 'Paso 2: Configuraci√≥n' primero.\")\n",
        "else:\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "    try:\n",
        "        print(f\"Verificando la existencia del bucket '{BUCKET_NAME}'...\")\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
        "        print(f\"‚úÖ Bucket '{bucket.name}' creado exitosamente en la regi√≥n {LOCATION}.\")\n",
        "    except exceptions.Conflict:\n",
        "        print(f\"‚úÖ El bucket '{BUCKET_NAME}' ya existe. Usando el bucket existente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Ocurri√≥ un error inesperado al interactuar con el bucket: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6is2ECfMhyKE"
      },
      "outputs": [],
      "source": [
        "# @title Paso 4: Sube tu PDF y lo guardamos en el Bucket\n",
        "# Este paso sube tu PDF al entorno de Colab y luego lo copia a nuestro bucket en la nube.\n",
        "# Se ha a√±adido una verificaci√≥n para no volver a subir el archivo si ya existe en el bucket.\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Nos aseguramos de que las variables necesarias existan\n",
        "if 'storage_client' not in locals() or 'BUCKET_NAME' not in locals() or 'ID_STUDENT' not in locals():\n",
        "    print(\"üö® ¬°Error! Por favor, ejecuta las celdas anteriores primero.\")\n",
        "else:\n",
        "    # Primero, sube el archivo PDF a este entorno de Colab.\n",
        "    print(\"Por favor, sube el archivo PDF con el que quieres chatear.\")\n",
        "    uploaded_pdf = files.upload()\n",
        "\n",
        "    if uploaded_pdf:\n",
        "        local_pdf_path = list(uploaded_pdf.keys())[0]\n",
        "\n",
        "        # Le damos un nombre √∫nico al archivo en el bucket para evitar conflictos\n",
        "        blob_name = f\"{ID_STUDENT}-{local_pdf_path}\"\n",
        "\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        # --- VERIFICACI√ìN DE EXISTENCIA ---\n",
        "        if blob.exists():\n",
        "            print(f\"\\n‚úÖ El archivo '{blob_name}' ya existe en el bucket. Saltando la subida.\")\n",
        "        else:\n",
        "            print(f\"\\nSubiendo '{local_pdf_path}' a gs://{BUCKET_NAME}/{blob_name}...\")\n",
        "            blob.upload_from_filename(local_pdf_path)\n",
        "            print(f\"‚úÖ ¬°Archivo subido exitosamente!\")\n",
        "\n",
        "        # Guardamos la ruta completa del archivo en la nube (URI) para el siguiente paso\n",
        "        GCS_PDF_URI = f\"gs://{BUCKET_NAME}/{blob_name}\"\n",
        "        print(f\"   URI del archivo a utilizar: {GCS_PDF_URI}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No se subi√≥ ning√∫n archivo. Por favor, ejecuta la celda de nuevo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC5sle0Bib6e"
      },
      "outputs": [],
      "source": [
        "# @title Paso 5: Crear o encontrar tu RAG Engine\n",
        "# Este paso busca un RAG Engine (Corpus) que te pertenezca.\n",
        "# - Si lo encuentra, lo usar√° directamente.\n",
        "# - Si no lo encuentra, crear√° uno nuevo e importar√° tu PDF.\n",
        "# Esto evita crear duplicados si ejecutas el cuaderno varias veces.\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview import rag\n",
        "import time\n",
        "\n",
        "# Nos aseguramos de que las variables necesarias existan\n",
        "if 'PROJECT_ID' not in locals() or 'LOCATION' not in locals() or 'RAG_CORPUS_DISPLAY_NAME' not in locals():\n",
        "    print(\"üö® ¬°Error! Por favor, ejecuta las celdas anteriores primero.\")\n",
        "else:\n",
        "    # Inicializamos el SDK de Vertex AI\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # --- VERIFICACI√ìN DE EXISTENCIA ---\n",
        "    print(f\"Buscando un RAG Corpus existente con el nombre: '{RAG_CORPUS_DISPLAY_NAME}'...\")\n",
        "\n",
        "    rag_corpora = rag.list_corpora()\n",
        "    found_corpus = None\n",
        "    for corpus in rag_corpora:\n",
        "        if corpus.display_name == RAG_CORPUS_DISPLAY_NAME:\n",
        "            found_corpus = corpus\n",
        "            break # Salimos del bucle en cuanto lo encontramos\n",
        "\n",
        "    # Si encontramos un corpus, usamos ese. Si no, creamos uno nuevo.\n",
        "    if found_corpus:\n",
        "        rag_corpus = found_corpus\n",
        "        print(f\"‚úÖ Corpus encontrado. Usando el corpus existente con ID: {rag_corpus.name}\")\n",
        "    else:\n",
        "        print(f\"No se encontr√≥ un corpus existente. Creando uno nuevo...\")\n",
        "        # Creamos el Corpus\n",
        "        rag_corpus = rag.create_corpus(display_name=RAG_CORPUS_DISPLAY_NAME)\n",
        "        print(f\"‚úÖ Corpus creado con el ID: {rag_corpus.name}\")\n",
        "\n",
        "        print(f\"\\nImportando tu PDF ({GCS_PDF_URI}) al nuevo corpus...\")\n",
        "        # Importamos el archivo desde el bucket al corpus\n",
        "        # Esta operaci√≥n solo se realiza cuando el corpus es nuevo.\n",
        "        rag.import_files(\n",
        "            rag_corpus.name,\n",
        "            [GCS_PDF_URI],\n",
        "            chunk_size=1024,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        print(\"\\n\\n‚úÖ ¬°Proceso de importaci√≥n iniciado!\")\n",
        "        print(\"¬°ATENCI√ìN! La indexaci√≥n del archivo puede tardar varios minutos.\")\n",
        "\n",
        "    # Guardamos el nombre del recurso para usarlo en el chat\n",
        "    RAG_CORPUS_RESOURCE_NAME = rag_corpus.name\n",
        "    print(f\"\\nListo para usar el RAG Engine con el recurso: {RAG_CORPUS_RESOURCE_NAME}\")\n",
        "    print(\"Puedes continuar a la siguiente celda, pero si el chat da error, espera un poco y vuelve a intentarlo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOfwCoSPjR-k"
      },
      "outputs": [],
      "source": [
        "# @title Paso 6: Inicializamos el asistente virtual\n",
        "\n",
        "import vertexai\n",
        "from vertexai import rag\n",
        "from vertexai.generative_models import GenerativeModel, Tool, GenerationConfig, HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# --- 1. Inicializaci√≥n de Vertex AI ---\n",
        "# Como en la documentaci√≥n, inicializamos el SDK.\n",
        "print(\"Inicializando el SDK de Vertex AI...\")\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "print(\"‚úÖ SDK de Vertex AI inicializado.\")\n",
        "\n",
        "# --- 2. Creaci√≥n de la Herramienta RAG ---\n",
        "# Utilizamos el patr√≥n exacto de la documentaci√≥n que proporcionaste.\n",
        "# La clave es usar `rag.VertexRagStore`.\n",
        "print(\"Creando la herramienta de recuperaci√≥n (RAG) con 'VertexRagStore'...\")\n",
        "rag_tool = Tool.from_retrieval(\n",
        "    retrieval=rag.Retrieval(\n",
        "        source=rag.VertexRagStore(\n",
        "            rag_resources=[\n",
        "                rag.RagResource(\n",
        "                    rag_corpus=RAG_CORPUS_RESOURCE_NAME,\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "print(\"‚úÖ Herramienta RAG creada exitosamente.\")\n",
        "\n",
        "# --- 3. Configuraci√≥n del Modelo Generativo ---\n",
        "# Instanciamos el modelo y le pasamos la herramienta RAG.\n",
        "print(\"Configurando el modelo GenerativeModel...\")\n",
        "# Usamos un modelo compatible con herramientas. gemini-1.5-pro es una excelente opci√≥n.\n",
        "model = GenerativeModel(\n",
        "    model_name=\"gemini-2.5-flash\",\n",
        "    tools=[rag_tool] # Conectamos la herramienta al modelo.\n",
        ")\n",
        "print(\"‚úÖ Modelo configurado con la herramienta RAG.\")\n",
        "\n",
        "# --- 4. Generamos respuesta ---\n",
        "query = input(\"Introduce tu pregunta: \")\n",
        "response = model.generate_content(\n",
        "  contents=query,\n",
        "  generation_config=GenerationConfig(\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=8192,\n",
        "    #stop_sequences=[\"STOP!\"],\n",
        "    response_logprobs=False,\n",
        "    #logprobs=3,\n",
        "  ),\n",
        "  safety_settings={\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "  },\n",
        ")\n",
        "\n",
        "print(f\"Respuesta: {response.candidates[0].to_dict()['content']['parts'][0]['text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5HEAqH0MLxY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfreVv1ZECWv"
      },
      "source": [
        "# Ejercicios (personas con experiencia en c√≥digo)\n",
        "\n",
        "A continuaci√≥n, para las personas con m√°s conocimiento del lengauje de programaci√≥n, sugerimos los siguientes ejercicios:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKdMNRplGd5N"
      },
      "source": [
        "## Ejercicio 1: Generaci√≥n de un dataset de preguntas para el sistema RAG\n",
        "\n",
        "**Objetivo**: Crear un fichero de preguntas de alta calidad en formato JSONL para evaluar de forma sistem√°tica el conocimiento de vuestro sistema sobre el PDF.\n",
        "\n",
        "**¬øPor qu√© es importante?**: Un buen sistema de evaluaci√≥n empieza con buenas preguntas. Si solo hacemos preguntas f√°ciles o sobre un √∫nico tema del documento, no tendremos una idea real de sus capacidades y limitaciones. Necesitamos un conjunto de preguntas diverso que cubra el documento en su totalidad.\n",
        "\n",
        "**Pasos a seguir:**\n",
        "1. Relee tu PDF con ojo cr√≠tico: Piensa como si fueras un examinador. ¬øQu√© tipo de preguntas har√≠as para saber si alguien ha entendido el documento a fondo?\n",
        "  * Preguntas de extracci√≥n directa: \"¬øCu√°l es la fecha de publicaci√≥n del documento?\" o \"¬øQu√© es el \"Modelo X\" seg√∫n la secci√≥n 3?\".\n",
        "  * Preguntas de resumen: \"Resume los puntos clave del cap√≠tulo 2\" o \"¬øCu√°les son las principales conclusiones del estudio?\".\n",
        "  * Preguntas de comparaci√≥n: \"Compara las metodolog√≠as A y B descritas en el ap√©ndice\".\n",
        "  * Preguntas \"dif√≠ciles\": Preguntas sobre detalles peque√±os, notas a pie de p√°gina o que requieran conectar informaci√≥n de distintas secciones.\n",
        "  * Preguntas fuera de dominio: Formula alguna pregunta que sabes que no est√° en el documento. La respuesta ideal del sistema deber√≠a ser algo como \"No tengo informaci√≥n sobre eso en el documento proporcionado\".\n",
        "\n",
        "2. Crea el fichero JSONL: JSONL (JSON Lines) es un formato de texto donde cada l√≠nea es un objeto JSON v√°lido. Es muy pr√°ctico para procesar grandes datasets. Tu fichero, que podr√≠as llamar preguntas_evaluacion.jsonl, tendr√° este aspecto:\n",
        "\n",
        "```json\n",
        "{\"unique_id\": \"pregunta_001\", \"query\": \"¬øCu√°l es el objetivo principal del documento?\"}\n",
        "{\"unique_id\": \"pregunta_002\", \"query\": \"Resume en tres puntos la secci√≥n sobre 'Resultados Preliminares'.\"}\n",
        "{\"unique_id\": \"pregunta_003\", \"query\": \"¬øQu√© autor se cita en la p√°gina 5 en relaci√≥n con la metodolog√≠a?\"}\n",
        "...\n",
        "{\"unique_id\": \"pregunta_050\", \"query\": \"¬øCu√°l es la predicci√≥n del tiempo para ma√±ana?\"}\n",
        "```\n",
        "\n",
        "  * unique_id: Un identificador √∫nico para cada pregunta. Te ser√° muy √∫til para rastrear los resultados.\n",
        "  * query: La pregunta que has formulado.\n",
        "\n",
        "**Pista de c√≥digo:** Puedes crear este archivo manualmente o usar un peque√±o script de Python para ayudarte a generarlo.\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Lista de preguntas que has formulado\n",
        "preguntas = [\n",
        "    \"¬øCu√°l es el objetivo principal del documento?\",\n",
        "    \"Resume en tres puntos la secci√≥n sobre 'Resultados Preliminares'.\",\n",
        "    \"¬øQu√© autor se cita en la p√°gina 5 en relaci√≥n con la metodolog√≠a?\",\n",
        "    # ... a√±ade aqu√≠ entre 30 y 50 preguntas\n",
        "]\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "nombre_archivo = \"queries_rag.jsonl\"\n",
        "\n",
        "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, pregunta in enumerate(preguntas):\n",
        "        # Creamos el diccionario para cada pregunta\n",
        "        record = {\n",
        "            \"unique_id\": f\"pregunta_{i+1:03d}\", # 'pregunta_001', 'pregunta_002', etc.\n",
        "            \"query\": pregunta\n",
        "        }\n",
        "        # Escribimos el objeto JSON como una l√≠nea en el archivo\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ ¬°Archivo '{nombre_archivo}' generado con {len(preguntas)} preguntas!\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JZuc6mjtA8G"
      },
      "outputs": [],
      "source": [
        "#  Mi c√≥digo para la generaci√≥n del archivo con preguntas - ML\n",
        "\n",
        "import json\n",
        "\n",
        "# Lista de preguntas que has formulado\n",
        "preguntas = [\n",
        "    \"1. ¬øCu√°l es el t√≠tulo completo del manual y qui√©n es su autor principal?\",\n",
        "    \"2. Seg√∫n el manual, ¬øcu√°ntos metros mide una milla marina?\",\n",
        "    \"3. ¬øQu√© significa 'cazar' una escota en el contexto de las maniobras a vela?\",\n",
        "    \"4. ¬øQu√© instrumento se utiliza para medir la presi√≥n atmosf√©rica?\",\n",
        "    \"5. ¬øQu√© nombre reciben los listones que arman la baluma de las velas?\",\n",
        "    \"6. ¬øCu√°l es el canal de llamada y para emergencias por convenci√≥n internacional en las radiocomunicaciones VHF?\",\n",
        "    \"7. ¬øQu√© es la pleamar?\",\n",
        "    \"8. Seg√∫n el RIPPA, ¬øqu√© significa espec√≠ficamente la palabra 'abordaje'?\",\n",
        "    \"9. ¬øCu√°l es el n√∫mero de tel√©fono directo de Prefectura Naval Argentina para urgencias?\",\n",
        "    \"10. ¬øCu√°l es el nombre de la publicaci√≥n del Servicio de Hidrograf√≠a Naval (SHN) que contiene el elenco de todas las cartas n√°uticas editadas?\",\n",
        "    \"11. Resuma las precauciones de seguridad esenciales que deben tomarse al cargar combustible en una embarcaci√≥n.\",\n",
        "    \"12. Describa brevemente los pasos principales para realizar la maniobra de 'hombre al agua' a vela, seg√∫n el manual.\",\n",
        "    \"13. Explique el concepto de 'equilibrio v√©lico' y qu√© indica si un barco tira mucho a orzar.\",\n",
        "    \"14. Sintetice qu√© informaci√≥n esencial proveen las cartas n√°uticas, adem√°s de los contornos de la costa.\",\n",
        "    \"15. Resuma las 'cinco reglas de oro' para navegar con viento fuerte o temporal inminente, seg√∫n el manual.\",\n",
        "    \"16. Describa el procedimiento para envergar un foque o velas de estay en general.\",\n",
        "    \"17. Sintetice qu√© es el balizamiento y sus componentes principales.\",\n",
        "    \"18. Resuma las caracter√≠sticas principales de una 'sudestada' en el R√≠o de la Plata, incluyendo su origen y duraci√≥n.\",\n",
        "    \"19. Describa el r√©gimen operativo de descarga de basura en navegaci√≥n mar√≠tima, diferenciando lo permitido a 3, 12 y 25 millas de la costa.\",\n",
        "    \"20. ¬øCu√°les son las cinco condiciones esenciales de una embarcaci√≥n que las aver√≠as pueden comprometer, y por qu√© son importantes?\",\n",
        "    \"21. Compare las diferencias entre la 'virada por avante' y la 'virada en redondo', indicando en qu√© direcci√≥n gira el barco en cada caso.\",\n",
        "    \"22. Diferencie un 'yawl' de un 'queche' en cuanto a la altura y ubicaci√≥n de su palo mesana.\",\n",
        "    \"23. Compare los principales usos de los cabos de Dacron, Nylon y polietileno/polipropileno, destacando una propiedad clave de cada uno.\",\n",
        "    \"24. Explique la diferencia entre 'correr el temporal' y 'capear' el temporal como estrategias de navegaci√≥n con mal tiempo.\",\n",
        "    \"25. Compare los tipos de anclas Danforth y arado (CQR/Delta) en t√©rminos de sus caracter√≠sticas de agarre y facilidad de estiba en la proa del barco.\",\n",
        "    \"26. Diferencie las 'mareas vivas o de sicigias' de las 'mareas muertas o de cuadratura', indicando cu√°ndo se producen y su amplitud.\",\n",
        "    \"27. Compare la navegaci√≥n con viento de popa redonda y la navegaci√≥n con viento de trav√©s en cuanto a la posici√≥n ideal de la vela mayor y el foque.\",\n",
        "    \"28. ¬øCu√°l es la diferencia principal entre un motor naftero y un motor di√©sel para veleros en t√©rminos de combustible y mecanismo de combusti√≥n?\",\n",
        "    \"29. Compare las caracter√≠sticas y duraci√≥n de un 'pampero' y una 'sudestada' en el R√≠o de la Plata.\",\n",
        "    \"30. Diferencie la 'luz de tope' de las 'luces de costado' de una embarcaci√≥n en navegaci√≥n nocturna, en cuanto a su color y el arco de visibilidad que cubren.\",\n",
        "    \"31. Al navegar de ce√±ida, ¬øpor qu√© es crucial que el timonel observe la parte del foque contra el estay y la mayor contra el palo, y qu√© acci√≥n debe tomar si la vela 'se pincha'?\",\n",
        "    \"32. Explique por qu√©, para un barco de la misma eslora, un aparejo tipo 'cutter' o 'queche' podr√≠a ser m√°s f√°cil de manipular para la tripulaci√≥n que un 'sloop'.\",\n",
        "    \"33. ¬øQu√© implicaciones tiene el efecto de 'rabeo' de la h√©lice en la maniobra de un velero con motor, especialmente al tomar arrancada o en aceleraciones bruscas, y c√≥mo puede mitigar su efecto en cruceros o lanchas con dos motores?\",\n",
        "    \"34. Si un barco se vara sobre fondo blando y el agua profunda queda a sotavento, ¬øcu√°l es la secuencia de maniobras a realizar para intentar zafar, y por qu√© el uso del tang√≥n puede ser fundamental en esta situaci√≥n?\",\n",
        "    \"35. El manual enfatiza la 'paciencia' como una 'virtud cardinal del marino' en visibilidad reducida. M√°s all√° de las precauciones t√©cnicas, ¬øqu√© actitud y decisiones se espera de un timonel prudente en estas condiciones?\",\n",
        "    \"36. ¬øPor qu√© el manual desaconseja el uso de bebidas alcoh√≥licas a una persona que sufre hipotermia severa, a pesar de que podr√≠a parecer que calientan el cuerpo?\",\n",
        "    \"37. Explique c√≥mo la acci√≥n meteorol√≥gica, espec√≠ficamente la direcci√≥n y fuerza del viento, puede causar una diferencia significativa entre la marea predicha y la observada en el R√≠o de la Plata, y c√≥mo puede un navegante obtener informaci√≥n actualizada sobre estas correcciones.\",\n",
        "    \"38. En una emergencia de 'hombre al agua' con viento fuerte, el manual especifica subir a la persona por la banda de sotavento. Explique la l√≥gica detr√°s de esta decisi√≥n y los peligros que se evitan.\",\n",
        "    \"39. Si un navegante no depende exclusivamente de la se√±al GPS para situarse, ¬øqu√© otros m√©todos de navegaci√≥n costera le permitir√≠an determinar su posici√≥n, y qu√© precauci√≥n se debe tener con la 'intersecci√≥n de l√≠neas' para asegurar la fiabilidad?\",\n",
        "    \"40. Al fondear, ¬øpor qu√© es importante no largar toda la cadena junta sobre el ancla y controlar su ca√≠da 'a medida que el barco retrocede y va pidiendo cadena'?\",\n",
        "    \"41. ¬øCu√°l es el costo aproximado de la matr√≠cula anual para un velero de 10 toneladas en Argentina?\",\n",
        "    \"42. ¬øCu√°l es la distancia en millas n√°uticas entre Buenos Aires y Mar del Plata?\",\n",
        "    \"43. ¬øQu√© tipo de extintor de incendios es el m√°s recomendado para veleros deportivos?\",\n",
        "    \"44. ¬øCu√°l es la profundidad promedio del R√≠o de la Plata en su desembocadura, m√°s all√° de la m√°xima de 25m?\",\n",
        "    \"45. ¬øQu√© materiales son los m√°s ecol√≥gicos para la construcci√≥n de cascos de veleros?\",\n",
        "    \"46. ¬øCu√°ntos tripulantes se recomiendan para un velero de 15 metros de eslora con aparejo de sloop?\",\n",
        "    \"47. ¬øQu√© tipo de fauna marina es caracter√≠stica del R√≠o de la Plata que pueda afectar la navegaci√≥n?\",\n",
        "    \"48. ¬øCu√°l es la cantidad ideal de aceite lubricante para un motor di√©sel de 30 HP?\",\n",
        "    \"49. ¬øCu√°l es la altitud de la ciudad de Buenos Aires sobre el nivel del mar?\",\n",
        "    \"50. ¬øCu√°les son las regulaciones sobre el uso de drones para fotograf√≠a a√©rea en el R√≠o de la Plata?\",\n",
        "]\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "nombre_archivo = \"queries_rag.jsonl\"\n",
        "\n",
        "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, pregunta in enumerate(preguntas):\n",
        "        # Creamos el diccionario para cada pregunta\n",
        "        record = {\n",
        "            \"unique_id\": f\"pregunta_{i+1:03d}\", # 'pregunta_001', 'pregunta_002', etc.\n",
        "            \"query\": pregunta\n",
        "        }\n",
        "        # Escribimos el objeto JSON como una l√≠nea en el archivo\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ ¬°Archivo '{nombre_archivo}' generado con {len(preguntas)} preguntas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6iPztemInpo"
      },
      "source": [
        "## Ejercicio 2: Generaci√≥n de respuestas con un script automatizado\n",
        "\n",
        "**Objetivo:** Convertir la l√≥gica del notebook en un script de Python reutilizable que lea tu fichero de preguntas y genere todas las respuestas de forma autom√°tica, aceptando par√°metros desde la l√≠nea de comandos.\n",
        "\n",
        "**¬øPor qu√© usar un script y la librer√≠a fire?**\n",
        "  * Automatizaci√≥n: Es la √∫nica forma viable de procesar decenas o cientos de preguntas.\n",
        "  * Reusabilidad: Puedes usar el mismo script para diferentes modelos, documentos o par√°metros.\n",
        "  * `fire`: Esta librer√≠a de Google convierte autom√°ticamente cualquier funci√≥n de Python en una interfaz de l√≠nea de comandos (CLI). Es incre√≠blemente sencilla y potente. En lugar de tener que modificar el c√≥digo para cambiar la temperatura, simplemente la pasas como un argumento: `python mi_script.py --temperature 0.8`\n",
        "\n",
        "**Pasos a seguir:**\n",
        "  1. Instalar `fire`:\n",
        "  ```python\n",
        "  python -m pip install fire\n",
        "  ```\n",
        "  2. Estructura tu script (`answers_rag.py`): El script debe encapsular la l√≥gica que ya ten√≠as en el notebook. La estructura ideal ser√≠a:\n",
        "    * Importar las librer√≠as necesarias (fire, json, vertexai, etc.).\n",
        "    * Definir una funci√≥n principal (p. ej., generar_evaluacion) que acepte como argumentos los par√°metros que quieres poder configurar (el fichero de entrada, el de salida, el modelo a usar, la temperatura, top_p, top_k, etc.).\n",
        "    * Dentro de esta funci√≥n:\n",
        "      * Inicializar Vertex AI.\n",
        "      * Crear la herramienta RAG (necesitar√°s el RAG_CORPUS_RESOURCE_NAME que obtuviste en el notebook).\n",
        "      * Configurar el modelo generativo con los par√°metros recibidos.\n",
        "      * Abrir el archivo de preguntas y el de salida.\n",
        "      * Iterar l√≠nea por l√≠nea sobre las preguntas.\n",
        "      * Para cada pregunta, llamar a model.generate_content().\n",
        "      * Crear un nuevo diccionario JSON que contenga la informaci√≥n original (`unique_id`, `query`) m√°s los nuevos datos (`answer`, `llm_model`, `temperature`, `top_p`, `top_k`, `max_output_tokens`).\n",
        "      * Guardar este nuevo diccionario en el fichero JSONL de resultados.\n",
        "    * Al final del script, a√±adir el c√≥digo para que `fire` exponga tu funci√≥n.\n",
        "\n",
        "**Pista de c√≥digo:**\n",
        "\n",
        "```python\n",
        "import fire\n",
        "import json\n",
        "import vertexai\n",
        "from vertexai.preview import rag\n",
        "from vertexai.generative_models import GenerativeModel, Tool, GenerationConfig\n",
        "import time\n",
        "\n",
        "# --- Pega aqu√≠ las constantes de tu proyecto ---\n",
        "PROJECT_ID = \"upgrade-hub-433600\"\n",
        "LOCATION = \"us-central1\"\n",
        "# ¬°MUY IMPORTANTE! Pega aqu√≠ el nombre del recurso que obtuviste en el paso 5 del notebook.\n",
        "RAG_CORPUS_RESOURCE_NAME = \"projects/1044524817197/locations/us-central1/ragCorpora/2443202797848494080\" # YA ACTUALIZADO CON MI RECURSO\n",
        "\n",
        "\n",
        "def generar_evaluacion(\n",
        "    input_file: str = \"queries_rag.jsonl\",\n",
        "    output_file: str = \"answers_rag.jsonl\",\n",
        "    model_name: str = \"gemini-2.5-flash\",\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 40,\n",
        "    max_output_tokens: int = 1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Genera respuestas para un conjunto de preguntas usando un sistema RAG\n",
        "    y guarda los resultados en un fichero JSONL.\n",
        "    \"\"\"\n",
        "    print(\"--- Iniciando el proceso de generaci√≥n de respuestas ---\")\n",
        "    print(f\"Usando el modelo: {model_name} con temperatura={temperature}\")\n",
        "\n",
        "    # 1. Inicializaci√≥n de Vertex AI\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # 2. Creaci√≥n de la Herramienta RAG\n",
        "    rag_tool = Tool.from_retrieval(\n",
        "        retrieval=rag.Retrieval(\n",
        "            source=rag.VertexRagStore(\n",
        "                rag_resources=[rag.RagResource(rag_corpus=RAG_CORPUS_RESOURCE_NAME)]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 3. Configuraci√≥n del Modelo Generativo\n",
        "    model = GenerativeModel(model_name=model_name, tools=[rag_tool])\n",
        "\n",
        "    generation_params = {\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"max_output_tokens\": max_output_tokens,\n",
        "    }\n",
        "    generation_config = GenerationConfig(**generation_params)\n",
        "\n",
        "    # 4. Procesamiento de archivos\n",
        "    print(f\"Leyendo preguntas de: {input_file}\")\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "         open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "        for line in f_in:\n",
        "            pregunta_data = json.loads(line)\n",
        "            query = pregunta_data[\"query\"]\n",
        "            print(f\"Generando respuesta para ID: {pregunta_data['unique_id']}...\")\n",
        "\n",
        "            # 5. Generar respuesta\n",
        "            response = model.generate_content(\n",
        "                contents=query,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            # 6. Guardar resultados\n",
        "            resultado = {\n",
        "                \"unique_id\": pregunta_data[\"unique_id\"],\n",
        "                \"query\": query,\n",
        "                \"answer\": response.text,\n",
        "                \"llm_model\": model_name,\n",
        "                \"temperature\": generation_params.temperature,\n",
        "                \"top_p\": generation_params.top_p,\n",
        "                \"top_k\": generation_params.top_k,\n",
        "                \"max_output_tokens\": generation_params.max_output_tokens,\n",
        "            }\n",
        "            f_out.write(json.dumps(resultado, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "            # Peque√±a pausa para no sobrecargar las APIs\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"--- Proceso completado. Resultados guardados en '{output_file}' ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fire.Fire(generar_evaluacion)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRVK0ezOEFIu"
      },
      "outputs": [],
      "source": [
        "!python -m pip install fire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BvsqxAnzYVk"
      },
      "outputs": [],
      "source": [
        "import fire\n",
        "import json\n",
        "import vertexai\n",
        "from vertexai.preview import rag\n",
        "from vertexai.generative_models import GenerativeModel, Tool, GenerationConfig\n",
        "import time\n",
        "import sys # Import the sys module\n",
        "import os # Import the os module\n",
        "\n",
        "# --- Pega aqu√≠ las constantes de tu proyecto ---\n",
        "PROJECT_ID = \"upgrade-hub-433600\"\n",
        "LOCATION = \"us-central1\"\n",
        "# ¬°MUY IMPORTANTE! Pega aqu√≠ el nombre del recurso que obtuviste en el paso 5 del notebook.\n",
        "# Make sure to use the correct RAG_CORPUS_RESOURCE_NAME from your successful run of cell fC5sle0Bib6e\n",
        "RAG_CORPUS_RESOURCE_NAME = \"projects/upgrade-hub-433600/locations/us-central1/ragCorpora/2443202797848494080\"\n",
        "\n",
        "\n",
        "def generar_evaluacion(\n",
        "    input_file: str = \"queries_rag.jsonl\",\n",
        "    output_file: str = \"answers_rag.jsonl\",\n",
        "    model_name: str = \"gemini-2.5-flash\",\n",
        "    temperature: float = 0.6,   #Jugar con este parametro a ver que pasa con las preguntas generadas y como las evalua el juez\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 40,\n",
        "    max_output_tokens: int = 1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Genera respuestas para un conjunto de preguntas usando un sistema RAG\n",
        "    y guarda los resultados en un fichero JSONL.\n",
        "    \"\"\"\n",
        "    print(\"--- Iniciando el proceso de generaci√≥n de respuestas ---\")\n",
        "    print(f\"Usando el modelo: {model_name} con temperature={temperature}\")\n",
        "    print(f\"Reading questions from: {input_file}\") # Print the input file path\n",
        "\n",
        "    # Check if input file exists and is not empty\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"üö® Error: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "    if os.path.getsize(input_file) == 0:\n",
        "        print(f\"üö® Error: Input file '{input_file}' is empty.\")\n",
        "        return\n",
        "\n",
        "    # 1. Inicializaci√≥n de Vertex AI\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # 2. Creaci√≥n de la Herramienta RAG\n",
        "    rag_tool = Tool.from_retrieval(\n",
        "        retrieval=rag.Retrieval(\n",
        "            source=rag.VertexRagStore(\n",
        "                rag_resources=[rag.RagResource(rag_corpus=RAG_CORPUS_RESOURCE_NAME)]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 3. Configuraci√≥n del Modelo Generativo\n",
        "    model = GenerativeModel(model_name=model_name, tools=[rag_tool])\n",
        "\n",
        "    generation_params = {\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"max_output_tokens\": max_output_tokens,\n",
        "    }\n",
        "    generation_config = GenerationConfig(**generation_params)\n",
        "\n",
        "    # 4. Processing files\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "         open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "        for line in f_in:\n",
        "            try:\n",
        "                pregunta_data = json.loads(line)\n",
        "                query = pregunta_data.get(\"query\")\n",
        "                unique_id = pregunta_data.get(\"unique_id\")\n",
        "\n",
        "                if not query:\n",
        "                    print(f\"  -> Warning: Skipping line with missing 'query' key: {line.strip()}\")\n",
        "                    continue\n",
        "                if not unique_id:\n",
        "                     print(f\"  -> Warning: Processing line with missing 'unique_id' key: {line.strip()}\")\n",
        "                     unique_id = \"unknown_id\" # Assign a default if missing\n",
        "\n",
        "\n",
        "                print(f\"Generating response for ID: {unique_id}...\")\n",
        "\n",
        "                # 5. Generate response\n",
        "                response = model.generate_content(\n",
        "                    contents=query,\n",
        "                    generation_config=generation_config\n",
        "                )\n",
        "\n",
        "                # 6. Save results\n",
        "                resultado = {\n",
        "                    \"unique_id\": unique_id,\n",
        "                    \"query\": query,\n",
        "                    \"answer\": response.text,\n",
        "                    \"llm_model\": model_name,\n",
        "                    \"temperature\": generation_params[\"temperature\"],\n",
        "                    \"top_p\": generation_params[\"top_p\"],\n",
        "                    \"top_k\": generation_params[\"top_k\"],\n",
        "                    \"max_output_tokens\": generation_params[\"max_output_tokens\"],\n",
        "                }\n",
        "                f_out.write(json.dumps(resultado, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                # Small pause to avoid overloading APIs\n",
        "                time.sleep(1)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"  -> Error decoding JSON line: {line.strip()} - {e}\")\n",
        "                # Optionally write an error record to the output file\n",
        "                error_record = {\"error\": \"JSONDecodeError\", \"line\": line.strip(), \"message\": str(e)}\n",
        "                f_out.write(json.dumps(error_record, ensure_ascii=False) + \"\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> An unexpected error occurred while processing line: {line.strip()} - {e}\")\n",
        "                # Optionally write an error record to the output file\n",
        "                error_record = {\"error\": \"ProcessingError\", \"line\": line.strip(), \"message\": str(e)}\n",
        "                f_out.write(json.dumps(error_record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "    print(f\"--- Process completed. Results saved to '{output_file}' ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This line checks if the script is being run in a Colab environment\n",
        "    # and passes an empty list of arguments to fire.Fire() if it is,\n",
        "    # preventing the \"FireExit: 2\" error.\n",
        "    if 'google.colab' in sys.modules:\n",
        "        fire.Fire(generar_evaluacion, []) # Pass an empty list of arguments\n",
        "    else:\n",
        "        fire.Fire(generar_evaluacion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12pKcw47KGx-"
      },
      "source": [
        "## Ejercicio 3: LLM as a Judge\n",
        "\n",
        "**Objetivo:** Utilizar un modelo de lenguaje de √∫ltima generaci√≥n (como Gemini 2.5 Pro) para evaluar la calidad de las respuestas generadas por tu sistema RAG, guardando las puntuaciones y justificaciones.\n",
        "\n",
        "**¬øPor qu√© es esto tan potente?**: Evaluar manualmente 50 respuestas es lento y subjetivo. Un LLM avanzado puede hacerlo de forma r√°pida, consistente y siguiendo criterios espec√≠ficos que le demos. Esta t√©cnica nos permite escalar la evaluaci√≥n de nuestros sistemas de IA.\n",
        "\n",
        "**Pasos a seguir:**\n",
        "\n",
        "1.  **Dise√±a el \"Prompt del Juez\":** Este es el paso m√°s importante. Debes crear un prompt que le d√© instrucciones claras al LLM sobre c√≥mo debe evaluar cada par de pregunta-respuesta. Un buen prompt es un arte, pero aqu√≠ tienes una plantilla robusta.\n",
        "\n",
        "  **Plantilla de Prompt para el Juez:**\n",
        "\n",
        "```text\n",
        "Eres un evaluador experto y meticuloso de sistemas de Pregunta-Respuesta (Q&A). Tu tarea es evaluar la calidad de una respuesta generada por un sistema de IA, bas√°ndote √∫nicamente en una pregunta dada.\n",
        "\n",
        "**Pregunta Original:**\n",
        "{query}\n",
        "\n",
        "**Respuesta Generada por la IA:**\n",
        "{answer}\n",
        "\n",
        "**Tus Criterios de Evaluaci√≥n son:**\n",
        "1.  **Relevancia y Coherencia:** ¬øLa respuesta aborda directamente la pregunta formulada? ¬øEs coherente y f√°cil de entender?\n",
        "2.  **Correcci√≥n y Fidelidad (si aplica):** Aunque no tienes acceso al documento original, eval√∫a si la respuesta parece factualmente plausible y si no contiene contradicciones obvias. Penaliza las respuestas que especulan o inventan informaci√≥n. Si la respuesta admite no saber la respuesta, consid√©ralo positivo.\n",
        "\n",
        "**Tu Tarea:**\n",
        "Proporciona una evaluaci√≥n en el siguiente formato JSON. No a√±adas nada antes o despu√©s del objeto JSON.\n",
        "\n",
        "{{\n",
        "  \"puntuacion_relevancia\": [Un n√∫mero entero del 1 al 5, donde 1 es \"totalmente irrelevante\" y 5 es \"perfectamente relevante y directa\"],\n",
        "  \"justificacion\": \"[Una explicaci√≥n clara y concisa (1-2 frases) que justifique tu puntuaci√≥n, explicando los puntos fuertes y d√©biles de la respuesta.]\"\n",
        "}}\n",
        "```\n",
        "\n",
        "2.  **Crea el script `llm_as_a_judge.py`:**\n",
        "    Este script ser√° muy similar al anterior. Usar√° `fire`, leer√° el fichero de resultados del ejercicio 2, y para cada resultado, llenar√° la plantilla del prompt y llamar√° a la API de Gemini 2.5 Pro.\n",
        "\n",
        "**C√≥digo de ejemplo (`llm_as_a_judge.py`):** A continuaci√≥n brindamos una sugerencia de c√≥digo:\n",
        "\n",
        "```python\n",
        "\n",
        "import fire\n",
        "import json\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "import time\n",
        "\n",
        "# --- Pega aqu√≠ las constantes de tu proyecto ---\n",
        "PROJECT_ID = \"upgrade-hub-433600\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# El prompt que hemos dise√±ado\n",
        "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
        "Eres un evaluador experto y meticuloso de sistemas de Pregunta-Respuesta (Q&A). Tu tarea es evaluar la calidad de una respuesta generada por un sistema de IA, bas√°ndote √∫nicamente en una pregunta dada.\n",
        "\n",
        "**Pregunta Original:**\n",
        "{query}\n",
        "\n",
        "**Respuesta Generada por la IA:**\n",
        "{answer}\n",
        "\n",
        "**Tus Criterios de Evaluaci√≥n son:**\n",
        "1.  **Relevancia y Coherencia:** ¬øLa respuesta aborda directamente la pregunta formulada? ¬øEs coherente y f√°cil de entender?\n",
        "2.  **Correcci√≥n y Fidelidad (si aplica):** Aunque no tienes acceso al documento original, eval√∫a si la respuesta parece factualmente plausible y si no contiene contradicciones obvias. Penaliza las respuestas que especulan o inventan informaci√≥n. Si la respuesta admite no saber la respuesta, consid√©ralo positivo.\n",
        "\n",
        "**Tu Tarea:**\n",
        "Proporciona una evaluaci√≥n en el siguiente formato JSON. No a√±adas nada antes o despu√©s del objeto JSON.\n",
        "\n",
        "{{\n",
        "  \"puntuacion_relevancia\": \"[Un n√∫mero entero del 1 al 5, donde 1 es 'totalmente irrelevante' y 5 es 'perfectamente relevante y directa']\",\n",
        "  \"justificacion\": \"[Una explicaci√≥n clara y concisa (1-2 frases) que justifique tu puntuaci√≥n, explicando los puntos fuertes y d√©biles de la respuesta.]\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def llm_judge(\n",
        "    input_file: str = \"answers_rag.jsonl\",\n",
        "    output_file: str = \"llm_judge_rag.jsonl\",\n",
        "    judge_model_name: str = \"gemini-2.5-pro\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Usa un LLM como juez para evaluar las respuestas generadas por el sistema RAG.\n",
        "    \"\"\"\n",
        "    print(f\"--- Iniciando evaluaci√≥n con el modelo juez: {judge_model_name} ---\")\n",
        "\n",
        "    # 1. Inicializaci√≥n de Vertex AI\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # 2. Configuraci√≥n del Modelo Juez\n",
        "    judge_model = GenerativeModel(model_name=judge_model_name)\n",
        "\n",
        "    # 3. Procesamiento de archivos\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "         open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "        for line in f_in:\n",
        "            result_data = json.loads(line)\n",
        "            print(f\"Juzgando respuesta para ID: {result_data['unique_id']}...\")\n",
        "\n",
        "            # Preparamos el prompt para el juez\n",
        "            prompt_final_juez = JUDGE_PROMPT_TEMPLATE.format(\n",
        "                query=result_data[\"query\"],\n",
        "                answer=result_data[\"answer\"]\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                # 4. Llamamos al modelo juez\n",
        "                response = judge_model.generate_content(\n",
        "                    prompt_final_juez,\n",
        "                    generation_config=GenerationConfig(temperature=0.0) # Queremos que el juez sea determinista\n",
        "                )\n",
        "\n",
        "                # 5. Parseamos la respuesta JSON del juez\n",
        "                judge_feedback = json.loads(response.text)\n",
        "\n",
        "                # Combinamos la data original con la evaluaci√≥n\n",
        "                final_record = {**result_data, \"evaluation\": judge_feedback}\n",
        "\n",
        "            except (json.JSONDecodeError, ValueError) as e:\n",
        "                print(f\"  -> Error al parsear la respuesta del juez para {result_data['unique_id']}: {e}\")\n",
        "                print(f\"  -> Respuesta recibida: {response.text}\")\n",
        "                final_record = {**result_data, \"evaluation\": {\"error\": \"Failed to parse judge response\", \"raw_response\": response.text}}\n",
        "            except Exception as e:\n",
        "                print(f\"  -> Ocurri√≥ un error inesperado: {e}\")\n",
        "                final_record = {**result_data, \"evaluation\": {\"error\": str(e)}}\n",
        "\n",
        "            # 6. Guardamos el resultado final\n",
        "            f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
        "            time.sleep(1) # Pausa\n",
        "\n",
        "    print(f\"--- Evaluaci√≥n completada. Resultados finales guardados en '{output_file}' ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fire.Fire(llm_judge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVSiXfru44xX"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"upgrade-hub-433600\"\n",
        "LOCATION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Aa6vduZB7e6L"
      },
      "outputs": [],
      "source": [
        "# Removed fire dependency for direct execution in Colab\n",
        "import json\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "import time\n",
        "# import sys # Removed sys import as it's not needed without fire\n",
        "\n",
        "\n",
        "# --- Pega aqu√≠ las constantes de tu proyecto ---\n",
        "PROJECT_ID = \"upgrade-hub-433600\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# El prompt que hemos dise√±ado\n",
        "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
        "Eres un evaluador experto y meticuloso de sistemas de Pregunta-Respuesta (Q&A). Tu tarea es evaluar la calidad de una respuesta generada por un sistema de IA, bas√°ndote √∫nicamente en una pregunta dada.\n",
        "\n",
        "**Pregunta Original:**\n",
        "{query}\n",
        "\n",
        "**Respuesta Generada por la IA:**\n",
        "{answer}\n",
        "\n",
        "**Tus Criterios de Evaluaci√≥n son:**\n",
        "1.  **Relevancia y Coherencia:** ¬øLa respuesta aborda directamente la pregunta formulada? ¬øEs coherente y f√°cil de entender?\n",
        "2.  **Correcci√≥n y Fidelidad (si aplica):** Aunque no tienes acceso al documento original, eval√∫a si la respuesta parece factualmente plausible y si no contiene contradicciones obvias. Penaliza las respuestas que especulan o inventan informaci√≥n. Si la respuesta admite no saber la respuesta, consid√©ralo positivo.\n",
        "\n",
        "**Tu Tarea:**\n",
        "Proporciona una evaluaci√≥n en el siguiente formato JSON. No a√±adas nada antes o despu√©s del objeto JSON.\n",
        "\n",
        "{{\n",
        "  \"puntuacion_relevancia\": \"[Un n√∫mero entero del 1 al 5, donde 1 es 'totalmente irrelevante' y 5 es 'perfectamente relevante y directa']\",\n",
        "  \"justificacion\": \"[Una explicaci√≥n clara y concisa (1-2 frases) que justifique tu puntuaci√≥n, explicando los puntos fuertes y d√©biles de la respuesta.]\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def llm_judge(\n",
        "    input_file: str = \"answers_rag.jsonl\",\n",
        "    output_file: str = \"llm_judge_rag.jsonl\",\n",
        "    judge_model_name: str = \"gemini-2.5-pro\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Usa un LLM como juez para evaluar las respuestas generadas por el sistema RAG.\n",
        "    \"\"\"\n",
        "    print(f\"--- Iniciando evaluaci√≥n con el modelo juez: {judge_model_name} ---\")\n",
        "\n",
        "    # 1. Inicializaci√≥n de Vertex AI\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # 2. Configuraci√≥n del Modelo Juez\n",
        "    judge_model = GenerativeModel(model_name=judge_model_name)\n",
        "\n",
        "    # 3. Procesamiento de archivos\n",
        "    print(f\"Leyendo respuestas de: {input_file}\")\n",
        "    try:\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "             open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "            for line in f_in:\n",
        "                try:\n",
        "                    result_data = json.loads(line)\n",
        "                    unique_id = result_data.get('unique_id', 'unknown_id') # Get with default to avoid KeyError\n",
        "                    query = result_data.get('query', '') # Get with default\n",
        "                    answer = result_data.get('answer', '') # Get with default\n",
        "\n",
        "                    if not query or not answer:\n",
        "                         print(f\"  -> Warning: Skipping line with missing 'query' or 'answer' key: {line.strip()}\")\n",
        "                         # Optionally write an error record for the skipped line\n",
        "                         error_record = {\"error\": \"Missing query or answer\", \"line\": line.strip()}\n",
        "                         f_out.write(json.dumps(error_record, ensure_ascii=False) + \"\\n\")\n",
        "                         continue\n",
        "\n",
        "\n",
        "                    print(f\"Juzgando respuesta para ID: {unique_id}...\")\n",
        "\n",
        "                    # Preparamos el prompt para el juez\n",
        "                    prompt_final_juez = JUDGE_PROMPT_TEMPLATE.format(\n",
        "                        query=query,\n",
        "                        answer=answer\n",
        "                    )\n",
        "\n",
        "                    try:\n",
        "                        # 4. Llamamos al modelo juez\n",
        "                        response = judge_model.generate_content(\n",
        "                            prompt_final_juez,\n",
        "                            generation_config=GenerationConfig(temperature=0.0) # Queremos que el juez sea determinista\n",
        "                        )\n",
        "\n",
        "                        # 5. Parseamos la respuesta JSON del juez\n",
        "                        # Handle cases where the response might not be valid JSON\n",
        "                        try:\n",
        "                            judge_feedback = json.loads(response.text)\n",
        "                        except json.JSONDecodeError:\n",
        "                            print(f\"  -> Warning: Judge response for {unique_id} was not valid JSON. Raw response: {response.text}\")\n",
        "                            judge_feedback = {\"error\": \"Invalid JSON response from judge\", \"raw_response\": response.text}\n",
        "\n",
        "\n",
        "                        # Combinamos la data original con la evaluaci√≥n\n",
        "                        final_record = {**result_data, \"evaluation\": judge_feedback}\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  -> Ocurri√≥ an error during judge generation or parsing for {unique_id}: {e}\")\n",
        "                        final_record = {**result_data, \"evaluation\": {\"error\": str(e)}}\n",
        "\n",
        "                    # 6. Guardamos el resultado final\n",
        "                    f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
        "                    time.sleep(1) # Pausa\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"  -> Error decoding JSON line: {line.strip()} - {e}\")\n",
        "                    # Optionally write an error record to the output file\n",
        "                    error_record = {\"error\": \"JSONDecodeError\", \"line\": line.strip(), \"message\": str(e)}\n",
        "                    f_out.write(json.dumps(error_record, ensure_ascii=False) + \"\\n\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  -> An unexpected error occurred while processing line: {line.strip()} - {e}\")\n",
        "                    # Optionally write an error record to the output file\n",
        "                    error_record = {\"error\": \"ProcessingError\", \"line\": line.strip(), \"message\": str(e)}\n",
        "                    f_out.write(json.dumps(error_record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "        print(f\"--- Evaluaci√≥n completada. Resultados finales guardados en '{output_file}' ---\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"üö® Error: Input file '{input_file}' not found. Please ensure you have run the previous step to generate 'answers_rag.jsonl'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"üö® An unexpected error occurred during file processing: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Call the function directly without fire\n",
        "    llm_judge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8AURdsCQ3Su"
      },
      "outputs": [],
      "source": [
        "#Instala matplotlib para hacer los gr√°ficos\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXFynMIxOwbj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Leer las puntuaciones del archivo de evaluaci√≥n\n",
        "scores = []\n",
        "with open(\"llm_judge_rag.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        eval_data = data.get(\"evaluation\", {})\n",
        "        score = eval_data.get(\"puntuacion_relevancia\")\n",
        "        # Si la puntuaci√≥n es string, convi√©rtela a int\n",
        "        try:\n",
        "            score_int = int(score)\n",
        "            scores.append(score_int)\n",
        "        except (TypeError, ValueError):\n",
        "            # Si no hay puntuaci√≥n v√°lida, agregar -1\n",
        "            scores.append(-1)\n",
        "\n",
        "# Contar la frecuencia de cada puntuaci√≥n\n",
        "score_counts = Counter(scores)\n",
        "score_labels = sorted(score_counts.keys())\n",
        "score_values = [score_counts[label] for label in score_labels]\n",
        "\n",
        "# Graficar\n",
        "plt.bar(score_labels, score_values, color='skyblue')\n",
        "plt.xlabel(\"Puntuaci√≥n de relevancia\")\n",
        "plt.ylabel(\"Cantidad de respuestas\")\n",
        "plt.title(\"Distribuci√≥n de calificaciones otorgadas por el juez LLM\")\n",
        "plt.xticks(score_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwsZrkDiOwbj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IfreVv1ZECWv"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
